{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06e0d4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from 'mnist_train.csv'...\n",
      "Loading test data from 'mnist_test.csv'...\n",
      "Training samples: 60000, Test samples: 10000\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.9262\n",
      "\n",
      "Training K-Nearest Neighbors (k=3)...\n",
      "KNN Accuracy: 0.9705\n",
      "\n",
      "Training Perceptron...\n",
      "Perceptron Accuracy: 0.8633\n",
      "\n",
      "Best model: KNN with accuracy 0.9705\n",
      "Saved best model 'KNN' to 'best_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load and normalize MNIST train and test datasets from CSV files.\n",
    "\n",
    "    Parameters:\n",
    "        train_path (str): Path to the training CSV file.\n",
    "        test_path (str): Path to the testing CSV file.\n",
    "\n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test (np.ndarray): Normalized features and labels.\n",
    "    \"\"\"\n",
    "    print(f\"Loading training data from '{train_path}'...\")\n",
    "    train_df = pd.read_csv(train_path, header=None)\n",
    "    X_train = train_df.iloc[:, 1:].values / 255.0\n",
    "    y_train = train_df.iloc[:, 0].values\n",
    "\n",
    "    print(f\"Loading test data from '{test_path}'...\")\n",
    "    test_df = pd.read_csv(test_path, header=None)\n",
    "    X_test = test_df.iloc[:, 1:].values / 255.0\n",
    "    y_test = test_df.iloc[:, 0].values\n",
    "\n",
    "    print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train Logistic Regression, KNN, and Perceptron classifiers and evaluate accuracy on test set.\n",
    "\n",
    "    Parameters:\n",
    "        X_train, y_train: Training data and labels.\n",
    "        X_test, y_test: Test data and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with model names as keys and tuples (model_instance, accuracy) as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    print(\"\\nTraining Logistic Regression...\")\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc = accuracy_score(y_test, lr_pred)\n",
    "    results['Logistic Regression'] = (lr, lr_acc)\n",
    "    print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining K-Nearest Neighbors (k=3)...\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    knn_acc = accuracy_score(y_test, knn_pred)\n",
    "    results['KNN'] = (knn, knn_acc)\n",
    "    print(f\"KNN Accuracy: {knn_acc:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining Perceptron...\")\n",
    "    perceptron = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "    perceptron.fit(X_train, y_train)\n",
    "    perc_pred = perceptron.predict(X_test)\n",
    "    perc_acc = accuracy_score(y_test, perc_pred)\n",
    "    results['Perceptron'] = (perceptron, perc_acc)\n",
    "    print(f\"Perceptron Accuracy: {perc_acc:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_best_model(results, accuracy_threshold=0.90, save_path='best_model.pkl'):\n",
    "    \"\"\"\n",
    "    Save the best performing model if it achieves or exceeds the accuracy threshold.\n",
    "\n",
    "    Parameters:\n",
    "        results (dict): Dictionary of trained models and their accuracies.\n",
    "        accuracy_threshold (float): Minimum accuracy required to save the model.\n",
    "        save_path (str): File path to save the model.\n",
    "    \"\"\"\n",
    "    best_model_name = max(results, key=lambda k: results[k][1])\n",
    "    best_model, best_acc = results[best_model_name]\n",
    "\n",
    "    print(f\"\\nBest model: {best_model_name} with accuracy {best_acc:.4f}\")\n",
    "    if best_acc >= accuracy_threshold:\n",
    "        joblib.dump(best_model, save_path)\n",
    "        print(f\"Saved best model '{best_model_name}' to '{save_path}'\")\n",
    "    else:\n",
    "        print(f\"No model achieved the accuracy threshold of {accuracy_threshold*100:.1f}%.\")\n",
    "\n",
    "def main():\n",
    "    # Paths to dataset CSV files\n",
    "    train_csv = 'mnist_train.csv'\n",
    "    test_csv = 'mnist_test.csv'\n",
    "\n",
    "    # Load data\n",
    "    X_train, y_train, X_test, y_test = load_data(train_csv, test_csv)\n",
    "\n",
    "    # Train models and evaluate\n",
    "    results = train_and_evaluate_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Save best model if meets threshold\n",
    "    save_best_model(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856343c2",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "* Loaded the MNIST dataset from CSV files containing flattened 28x28 grayscale images.\n",
    "* Normalized pixel values to the range \\[0,1].\n",
    "* Implemented three classification algorithms:\n",
    "\n",
    "  * Logistic Regression (`max_iter=1000`)\n",
    "  * K-Nearest Neighbors (k=3)\n",
    "  * Perceptron (`max_iter=1000`, `tol=1e-3`)\n",
    "* Trained each model on 60,000 training samples and evaluated on 10,000 test samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Results\n",
    "\n",
    "| Model               | Test Accuracy |\n",
    "| ------------------- | ------------- |\n",
    "| Logistic Regression | 92.62%        |\n",
    "| K-Nearest Neighbors | 97.05%        |\n",
    "| Perceptron          | 86.33%        |\n",
    "\n",
    "* The KNN model achieved the highest accuracy of 97.05% on the test set.\n",
    "* The best model (KNN) was saved for future use.\n",
    "\n",
    "---\n",
    "\n",
    "### Analysis\n",
    "\n",
    "* KNN outperformed both Logistic Regression and Perceptron on this dataset.\n",
    "* Logistic Regression performed well with over 90% accuracy but was less accurate than KNN.\n",
    "* Perceptron showed the lowest accuracy, likely due to its linear nature and simplicity.\n",
    "* The high accuracy of KNN suggests that instance-based learning is effective for MNIST digit classification.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The K-Nearest Neighbors classifier is the best-performing model among those tested, achieving 97.05% accuracy on MNIST. Logistic Regression also performed strongly but was less accurate. Perceptron lagged behind. These results demonstrate that classical ML algorithms can effectively classify handwritten digits without deep learning, meeting and exceeding the 90% accuracy target."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonCVI620NSA",
   "language": "python",
   "name": "cvi620nsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
